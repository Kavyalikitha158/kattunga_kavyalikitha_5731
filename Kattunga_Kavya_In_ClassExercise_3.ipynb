{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdRwkJBn70nX"
      },
      "source": [
        "# **INFO5731 In-class Exercise 3**\n",
        "\n",
        "The purpose of this exercise is to explore various aspects of text analysis, including feature extraction, feature selection, and text similarity ranking.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of Friday, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARqm7u6B70ne"
      },
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting **text classification or text mining task** and explain what kind of features might be useful for you to build the machine learning model. List your features and explain why these features might be helpful. You need to list at least five different types of features. **Your dataset must be text.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "VAZj4PHB70nf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "974ddaa3-510e-4cf8-8a57-35e2b57273bb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIdentifying Satirical News Articles:\\nSatirical news pieces frequently imitate official news while adding ludicrous or inflated details to make fun of or comment on current affairs. Developing a machine that can automatically discern between news stories that are satirical and those that are not is the task at hand.\\na)Sentiment analysis: This feature encapsulates the article\\'s emotional tone, be it neutral, positive, or negative. Satirical articles frequently have an extreme positive or negative attitude that may not be consistent with the serious tone of regular news, as well as a sardonic or exaggerated tone.\\nb)Lexical Expressions: Measures the number of unique words used in the article in relation to its total word count. While non-satirical news typically sticks to a more official and standardized terminology, satirical pieces may utilize more imaginative or exaggerated language.\\nc)Frequency of Word/Term: This tool monitors the frequency with which specific keywords or phrases—both common and uncommon words—appear. Certain terms or phrases—such as \"aliens\" or \"unicorns\"—that are humorous, ironic, or ridiculous may appear in satirical pieces. Such words would be less common in non-satirical news pieces.\\nd)N-grams(tri- and bi-grams): N-grams are collections of words or phrases that appear in a text in a particular order. Satirical news frequently uses clever, ironic, or hilarious word combinations. These patterns, which would not be apparent with only one word (e.g., \"world\\'s greatest,\" \"according to sources\"), can be identified by analyzing bi-grams and tri-grams.\\ne)Named Entities Recognition(NER): Proper nouns, such names of individuals, locations, businesses, etc., are identified by this attribute. Non-satirical stories usually report on actual events, whereas satirical articles frequently exaggerate real-life characters or events (e.g., turning a political figure into a superhero). Satire can be identified by variations in the usage of named entities.\\nf)Subjectivity Score: This quantifies the extent to which a text is objective or subjective. Non-satirical news should be more objective and accurate, whereas satirical news frequently includes ideas, feelings, and fictitious components, making it very subjective.\\n\\nEach of these characteristics draws attention to the language or topical distinctions between authentic news and satire. Sentiment analysis and subjectivity capture the tone and perspective, lexical diversity and term frequency highlight the language style, and NER and n-grams reveal the structure of the content. These characteristics work together to highlight the minute distinctions in the ways satire manipulates reality for humorous effect.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Identifying Satirical News Articles:\n",
        "Satirical news pieces frequently imitate official news while adding ludicrous or inflated details to make fun of or comment on current affairs. Developing a machine that can automatically discern between news stories that are satirical and those that are not is the task at hand.\n",
        "a)Sentiment analysis: This feature encapsulates the article's emotional tone, be it neutral, positive, or negative. Satirical articles frequently have an extreme positive or negative attitude that may not be consistent with the serious tone of regular news, as well as a sardonic or exaggerated tone.\n",
        "b)Lexical Expressions: Measures the number of unique words used in the article in relation to its total word count. While non-satirical news typically sticks to a more official and standardized terminology, satirical pieces may utilize more imaginative or exaggerated language.\n",
        "c)Frequency of Word/Term: This tool monitors the frequency with which specific keywords or phrases—both common and uncommon words—appear. Certain terms or phrases—such as \"aliens\" or \"unicorns\"—that are humorous, ironic, or ridiculous may appear in satirical pieces. Such words would be less common in non-satirical news pieces.\n",
        "d)N-grams(tri- and bi-grams): N-grams are collections of words or phrases that appear in a text in a particular order. Satirical news frequently uses clever, ironic, or hilarious word combinations. These patterns, which would not be apparent with only one word (e.g., \"world's greatest,\" \"according to sources\"), can be identified by analyzing bi-grams and tri-grams.\n",
        "e)Named Entities Recognition(NER): Proper nouns, such names of individuals, locations, businesses, etc., are identified by this attribute. Non-satirical stories usually report on actual events, whereas satirical articles frequently exaggerate real-life characters or events (e.g., turning a political figure into a superhero). Satire can be identified by variations in the usage of named entities.\n",
        "f)Subjectivity Score: This quantifies the extent to which a text is objective or subjective. Non-satirical news should be more objective and accurate, whereas satirical news frequently includes ideas, feelings, and fictitious components, making it very subjective.\n",
        "\n",
        "Each of these characteristics draws attention to the language or topical distinctions between authentic news and satire. Sentiment analysis and subjectivity capture the tone and perspective, lexical diversity and term frequency highlight the language style, and NER and n-grams reveal the structure of the content. These characteristics work together to highlight the minute distinctions in the ways satire manipulates reality for humorous effect.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEUjBE6C70nf"
      },
      "source": [
        "## Question 2 (10 Points)\n",
        "Write python code to extract these features you discussed above. You can collect a few sample text data for the feature extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "EoQX5s4O70nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0edecd-5e52-47a0-995d-c95fd5870221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features for text 1:\n",
            "polarity: -0.29583333333333334\n",
            "subjectivity: 0.48333333333333334\n",
            "lexical_diversity: 1.0\n",
            "word_frequency: {'actually': 1, 'announce': 1, 'breakthrough': 1, 'discovery': 1, 'experiment': 1, 'gone': 1, 'internet': 1, 'scientists': 1, 'social': 1, 'the': 1, 'was': 1, 'wrong': 1}\n",
            "bi_grams: [('scientists', 'announce'), ('announce', 'breakthrough'), ('breakthrough', 'discovery'), ('discovery', ':'), (':', 'the'), ('the', 'internet'), ('internet', 'was'), ('was', 'actually'), ('actually', 'a'), ('a', 'social'), ('social', 'experiment'), ('experiment', 'gone'), ('gone', 'wrong'), ('wrong', '!')]\n",
            "tri_grams: [('scientists', 'announce', 'breakthrough'), ('announce', 'breakthrough', 'discovery'), ('breakthrough', 'discovery', ':'), ('discovery', ':', 'the'), (':', 'the', 'internet'), ('the', 'internet', 'was'), ('internet', 'was', 'actually'), ('was', 'actually', 'a'), ('actually', 'a', 'social'), ('a', 'social', 'experiment'), ('social', 'experiment', 'gone'), ('experiment', 'gone', 'wrong'), ('gone', 'wrong', '!')]\n",
            "named_entities: [('Announce Breakthrough', 'PERSON')]\n",
            "\n",
            "\n",
            "Features for text 2:\n",
            "polarity: 0.16818181818181818\n",
            "subjectivity: 0.32727272727272727\n",
            "lexical_diversity: 1.0\n",
            "word_frequency: {'announces': 1, 'economic': 1, 'for': 1, 'government': 1, 'healthcare': 1, 'improving': 1, 'infrastructure': 1, 'new': 1, 'plan': 1, 'the': 1}\n",
            "bi_grams: [('the', 'government'), ('government', 'announces'), ('announces', 'a'), ('a', 'new'), ('new', 'economic'), ('economic', 'plan'), ('plan', 'for'), ('for', 'improving'), ('improving', 'healthcare'), ('healthcare', 'infrastructure'), ('infrastructure', '.')]\n",
            "tri_grams: [('the', 'government', 'announces'), ('government', 'announces', 'a'), ('announces', 'a', 'new'), ('a', 'new', 'economic'), ('new', 'economic', 'plan'), ('economic', 'plan', 'for'), ('plan', 'for', 'improving'), ('for', 'improving', 'healthcare'), ('improving', 'healthcare', 'infrastructure'), ('healthcare', 'infrastructure', '.')]\n",
            "named_entities: []\n",
            "\n",
            "\n",
            "Features for text 3:\n",
            "polarity: 0.0\n",
            "subjectivity: 0.125\n",
            "lexical_diversity: 1.0\n",
            "word_frequency: {'agree': 1, 'all': 1, 'championship': 1, 'disputes': 1, 'future': 1, 'leaders': 1, 'paper': 1, 'rock': 1, 'scissors': 1, 'settle': 1, 'to': 1, 'with': 1, 'world': 1}\n",
            "bi_grams: [('world', 'leaders'), ('leaders', 'agree'), ('agree', 'to'), ('to', 'settle'), ('settle', 'all'), ('all', 'future'), ('future', 'disputes'), ('disputes', 'with'), ('with', 'rock-paper-scissors'), ('rock-paper-scissors', 'championship')]\n",
            "tri_grams: [('world', 'leaders', 'agree'), ('leaders', 'agree', 'to'), ('agree', 'to', 'settle'), ('to', 'settle', 'all'), ('settle', 'all', 'future'), ('all', 'future', 'disputes'), ('future', 'disputes', 'with'), ('disputes', 'with', 'rock-paper-scissors'), ('with', 'rock-paper-scissors', 'championship')]\n",
            "named_entities: []\n",
            "\n",
            "\n",
            "Features for text 4:\n",
            "polarity: 0.0\n",
            "subjectivity: 0.0\n",
            "lexical_diversity: 1.0\n",
            "word_frequency: {'about': 1, 'authorities': 1, 'cities': 1, 'coastal': 1, 'flood': 1, 'in': 1, 'local': 1, 'residents': 1, 'rising': 1, 'risks': 1, 'warn': 1}\n",
            "bi_grams: [('local', 'authorities'), ('authorities', 'warn'), ('warn', 'residents'), ('residents', 'about'), ('about', 'rising'), ('rising', 'flood'), ('flood', 'risks'), ('risks', 'in'), ('in', 'coastal'), ('coastal', 'cities'), ('cities', '.')]\n",
            "tri_grams: [('local', 'authorities', 'warn'), ('authorities', 'warn', 'residents'), ('warn', 'residents', 'about'), ('residents', 'about', 'rising'), ('about', 'rising', 'flood'), ('rising', 'flood', 'risks'), ('flood', 'risks', 'in'), ('risks', 'in', 'coastal'), ('in', 'coastal', 'cities'), ('coastal', 'cities', '.')]\n",
            "named_entities: []\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "import nltk\n",
        "import spacy\n",
        "from textblob import TextBlob\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "import en_core_web_sm\n",
        "\n",
        "# Downloading necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "# Load spaCy model for NER\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "# Sample text data: Satirical and Non-satirical\n",
        "texts = [\n",
        "    \"Scientists Announce Breakthrough Discovery: The Internet Was Actually a Social Experiment Gone Wrong!\",\n",
        "    \"The government announces a new economic plan for improving healthcare infrastructure.\",\n",
        "    \"World Leaders Agree to Settle All Future Disputes with Rock-Paper-Scissors Championship\",\n",
        "    \"Local authorities warn residents about rising flood risks in coastal cities.\"\n",
        "]\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(text):\n",
        "    # Initialize a dictionary to hold the features\n",
        "    features = {}\n",
        "\n",
        "    # 1. Sentiment Analysis using TextBlob\n",
        "    sentiment = TextBlob(text).sentiment\n",
        "    features['polarity'] = sentiment.polarity  # Positive/Negative\n",
        "    features['subjectivity'] = sentiment.subjectivity  # Subjective vs Objective\n",
        "\n",
        "    # 2. Lexical Diversity\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    unique_tokens = set(tokens)\n",
        "    lexical_diversity = len(unique_tokens) / len(tokens) if len(tokens) > 0 else 0\n",
        "    features['lexical_diversity'] = lexical_diversity\n",
        "\n",
        "    # 3. Word/Term Frequency using CountVectorizer\n",
        "    vectorizer = CountVectorizer()\n",
        "    word_counts = vectorizer.fit_transform([text])\n",
        "    word_freq = dict(zip(vectorizer.get_feature_names_out(), word_counts.toarray()[0]))\n",
        "    features['word_frequency'] = word_freq\n",
        "\n",
        "    # 4. N-grams (bi-grams and tri-grams)\n",
        "    bi_grams = list(ngrams(tokens, 2))\n",
        "    tri_grams = list(ngrams(tokens, 3))\n",
        "    features['bi_grams'] = bi_grams\n",
        "    features['tri_grams'] = tri_grams\n",
        "\n",
        "    # 5. Named Entity Recognition using spaCy\n",
        "    doc = nlp(text)\n",
        "    named_entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    features['named_entities'] = named_entities\n",
        "\n",
        "    return features\n",
        "\n",
        "# Extract features for each text\n",
        "for i, text in enumerate(texts):\n",
        "    print(f\"Features for text {i+1}:\")\n",
        "    features = extract_features(text)\n",
        "    for key, value in features.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oSK4soH70nf"
      },
      "source": [
        "## Question 3 (10 points):\n",
        "Use any of the feature selection methods mentioned in this paper \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019). Feature selection for text classification: A review. Multimedia Tools & Applications, 78(3).\"\n",
        "\n",
        "Select the most important features you extracted above, rank the features based on their importance in the descending order."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2CRuXfV570ng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "dc51d09f-6764-47ec-b6fc-620bc7314e68"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAccording to the publication \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019),\" feature selection techniques for text classification encompass methods such as:\\nThe Chi-Square Test (X2) determines which properties (terms) are more likely to occur in a given class by measuring the deviation between the predicted and observed frequencies of terms.\\nInformation Gain (IG) measures the amount of information a feature adds to the target class prediction.\\nMutual Information (MI) calculates the extent to which being aware of a feature\\'s value lessens ambiguity regarding the class.\\nTerm Frequency-Inverse Document Frequency (TF-IDF) is a method assigns greater weight to terms that are more unique by weighing a term\\'s frequency throughout the corpus in relation to its rarity.\\n\\nI plan to employ Information Gain (IG) as my feature selection strategy in order to assign a lower priority to the extracted features. IG gauges how much a feature contributes to reducing ambiguity when identifying satirist or non-satirical articles. The features can be ranked conceptually according to the degree to which they are likely to aid in differentiating satirical texts from non-satirical ones.\\n\\nSubjectivity Score: While non-satirical news is more objective, satirical articles are typically very subjective.\\nRank 1: Offers the most distinct line between factual reporting and a fictional or satirical tone.\\n\\nSentiment Analysis: While regular news is typically neutral, satirical news frequently conveys strong sentiment.\\nRank 2: Able to recognize sarcastic or exaggerated tones that are frequently found in satire.\\n\\nNamed Entities Recognition(NER): Non-satirical news concentrates on real-world entities, whereas satirical pieces may feature made-up characters or exaggerated names.\\nRank 3: Assists in determining if the individuals, places, or events in the article are real or have been exaggerated or faked.\\n\\nLexical Diversity: Compared to official news pieces, satirical articles may employ a more inventive and varied vocabulary.\\nRank 4: Assists in setting satirical creative writing apart from news language.\\n\\nN-grams (Bi- and Tri-grams): Due to irony or comedy, some word combinations or phrases may only be found in satire.\\nRank 5: Describes the stylistic distinctions between the information conveyed by satirist and non-satire.\\n\\nWord/Term Frequency: those used in satirical writing may be different from those used in non-satirical writing.\\nRank 6: Significant but sometimes distracting, particularly for widely used terms.\\n\\nBy focusing on high-ranking features like subjectivity and Sentiment analysis, we can likely improve the performance of a model in classifying satirical vs. non-satirical texts.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "'''\n",
        "According to the publication \"Deng, X., Li, Y., Weng, J., & Zhang, J. (2019),\" feature selection techniques for text classification encompass methods such as:\n",
        "The Chi-Square Test (X2) determines which properties (terms) are more likely to occur in a given class by measuring the deviation between the predicted and observed frequencies of terms.\n",
        "Information Gain (IG) measures the amount of information a feature adds to the target class prediction.\n",
        "Mutual Information (MI) calculates the extent to which being aware of a feature's value lessens ambiguity regarding the class.\n",
        "Term Frequency-Inverse Document Frequency (TF-IDF) is a method assigns greater weight to terms that are more unique by weighing a term's frequency throughout the corpus in relation to its rarity.\n",
        "\n",
        "I plan to employ Information Gain (IG) as my feature selection strategy in order to assign a lower priority to the extracted features. IG gauges how much a feature contributes to reducing ambiguity when identifying satirist or non-satirical articles. The features can be ranked conceptually according to the degree to which they are likely to aid in differentiating satirical texts from non-satirical ones.\n",
        "\n",
        "Subjectivity Score: While non-satirical news is more objective, satirical articles are typically very subjective.\n",
        "Rank 1: Offers the most distinct line between factual reporting and a fictional or satirical tone.\n",
        "\n",
        "Sentiment Analysis: While regular news is typically neutral, satirical news frequently conveys strong sentiment.\n",
        "Rank 2: Able to recognize sarcastic or exaggerated tones that are frequently found in satire.\n",
        "\n",
        "Named Entities Recognition(NER): Non-satirical news concentrates on real-world entities, whereas satirical pieces may feature made-up characters or exaggerated names.\n",
        "Rank 3: Assists in determining if the individuals, places, or events in the article are real or have been exaggerated or faked.\n",
        "\n",
        "Lexical Diversity: Compared to official news pieces, satirical articles may employ a more inventive and varied vocabulary.\n",
        "Rank 4: Assists in setting satirical creative writing apart from news language.\n",
        "\n",
        "N-grams (Bi- and Tri-grams): Due to irony or comedy, some word combinations or phrases may only be found in satire.\n",
        "Rank 5: Describes the stylistic distinctions between the information conveyed by satirist and non-satire.\n",
        "\n",
        "Word/Term Frequency: those used in satirical writing may be different from those used in non-satirical writing.\n",
        "Rank 6: Significant but sometimes distracting, particularly for widely used terms.\n",
        "\n",
        "By focusing on high-ranking features like subjectivity and Sentiment analysis, we can likely improve the performance of a model in classifying satirical vs. non-satirical texts.\n",
        "\n",
        "'''\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nZGAOwl70ng"
      },
      "source": [
        "## Question 4 (10 points):\n",
        "Write python code to rank the text based on text similarity. Based on the text data you used for question 2, design a query to match the most relevant docments. Please use the BERT model to represent both your query and the text data, then calculate the cosine similarity between the query and each text in your data. Rank the similary with descending order."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sample texts (from Question 2)\n",
        "texts = [\n",
        "    \"Scientists Announce Breakthrough Discovery: The Internet Was Actually a Social Experiment Gone Wrong!\",\n",
        "    \"The government announces a new economic plan for improving healthcare infrastructure.\",\n",
        "    \"World Leaders Agree to Settle All Future Disputes with Rock-Paper-Scissors Championship\",\n",
        "    \"Local authorities warn residents about rising flood risks in coastal cities.\"\n",
        "]\n",
        "\n",
        "# Query (the document to match with)\n",
        "query = \"Internet and social experiments gone wrong\"\n",
        "\n",
        "# Function to encode the text using BERT and get the mean of token embeddings\n",
        "def encode_text(text):\n",
        "    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embeddings = outputs.last_hidden_state\n",
        "    # Get the mean of the token embeddings to represent the sentence\n",
        "    sentence_embedding = torch.mean(embeddings, dim=1)\n",
        "    return sentence_embedding\n",
        "\n",
        "# Encode the query and the text data\n",
        "query_embedding = encode_text(query)\n",
        "text_embeddings = [encode_text(text) for text in texts]\n",
        "\n",
        "# Calculate cosine similarity between the query and each text\n",
        "similarity_scores = [cosine_similarity(query_embedding, text_embedding)[0][0] for text_embedding in text_embeddings]\n",
        "\n",
        "# Rank the texts by similarity score in descending order\n",
        "ranked_texts = sorted(zip(similarity_scores, texts), key=lambda x: x[0], reverse=True)\n",
        "\n",
        "# Print the ranked texts and their similarity scores\n",
        "for score, text in ranked_texts:\n",
        "    print(f\"Similarity Score: {score:.4f} | Text: {text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHfFh1CmmdyX",
        "outputId": "1e21e340-ad1c-41ab-e442-adcab34ac4ab"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Similarity Score: 0.7883 | Text: Scientists Announce Breakthrough Discovery: The Internet Was Actually a Social Experiment Gone Wrong!\n",
            "Similarity Score: 0.6096 | Text: The government announces a new economic plan for improving healthcare infrastructure.\n",
            "Similarity Score: 0.5990 | Text: World Leaders Agree to Settle All Future Disputes with Rock-Paper-Scissors Championship\n",
            "Similarity Score: 0.5655 | Text: Local authorities warn residents about rising flood risks in coastal cities.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "VEs-OoDEhTW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on this exercise**\n",
        "\n",
        "Please provide your thoughts and feedback on the exercises you completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on extracting features from text data. What were the key concepts or techniques you found most beneficial in understanding the process?\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in completing this exercise?\n",
        "\n",
        "Relevance to Your Field of Study: How does this exercise relate to the field of NLP?\n",
        "\n",
        "**(Your submission will not be graded if this question is left unanswered)**\n",
        "\n"
      ],
      "metadata": {
        "id": "IUKC7suYhVl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Learning Experience: I found working on feature extraction from text data to be incredibly enlightening. I discovered how crucial it is to recognize the differences between various texts. Through the use of tools such as TextBlob for sentiment analysis and spaCy for named entity identification, I was able to get practical expertise in spotting patterns that I otherwise would not have observed. The ability of characteristics like subjectivity and sentiment analysis to disclose concealed tones in a text, particularly for differentiating between humor and serious content, was what most impressed me. I also discovered that lexical diversity reveals the formality or creativity of a piece of writing.\n",
        "\n",
        "Challenges Encountered: Determining which elements would be most crucial for text classification was one of the most difficult tasks. Selecting the ideal characteristics isn't always easy because there are so many options. For example, it was difficult to determine which word combinations were most important when extracting bi- and tri-grams. Additionally, at first, utilizing a pre-trained model for text similarity—such as BERT—was a little complicated because I had to learn how to apply cosine similarity and comprehend how embeddings function.\n",
        "\n",
        "Relevance to my Field of Study: Natural Language Processing (NLP), which is a significant component of cybersecurity and data science, is closely related to this exercise. In data science, text feature extraction is essential to improving predictions, while in cybersecurity, pattern recognition in text can aid in identifying phishing emails or other fraudulent communications. Because it increases the accuracy of text-based machine learning models, which is extremely relevant in both domains, learning how to extract and rank features is crucial.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "CAq0DZWAhU9m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "81d13faa-d73b-4661-dcc5-ec8352363e52"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nLearning Experience: I found working on feature extraction from text data to be incredibly enlightening. I discovered how crucial it is to recognize the differences between various texts. Through the use of tools such as TextBlob for sentiment analysis and spaCy for named entity identification, I was able to get practical expertise in spotting patterns that I otherwise would not have observed. The ability of characteristics like subjectivity and sentiment analysis\\xa0to disclose concealed tones in a text, particularly for differentiating between humor and serious content, was what most impressed me. I also discovered that lexical diversity reveals the formality or creativity of a piece of writing.\\n\\nChallenges Encountered: Determining which elements would be most crucial for text classification was one of the most difficult tasks. Selecting the ideal characteristics isn't always easy because there are so many options. For example, it was difficult to determine which word combinations were most important when extracting bi- and tri-grams. Additionally, at first, utilizing a pre-trained model for text similarity—such as BERT—was a little complicated because I had to learn how to apply cosine similarity and comprehend how embeddings function.\\n\\nRelevance to my Field of Study: Natural Language Processing (NLP), which is a significant component of cybersecurity and data science, is closely related to this exercise. In data science, text feature extraction is essential to improving predictions, while in cybersecurity, pattern recognition in text can aid in identifying phishing emails or other fraudulent communications. Because it increases the accuracy of text-based machine learning models, which is extremely relevant in both domains, learning how to extract and rank features is crucial.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}