{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DymRJbxDBCnf"
      },
      "source": [
        "# **INFO5731 In-class Exercise 2**\n",
        "\n",
        "The purpose of this exercise is to understand users' information needs, and then collect data from different sources for analysis by implementing web scraping using Python.\n",
        "\n",
        "**Expectations**:\n",
        "*   Students are expected to complete the exercise during lecture period to meet the active participation criteria of the course.\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "**Total points**: 40\n",
        "\n",
        "**Deadline**: This in-class exercise is due at the end of the day tomorrow, at 11:59 PM.\n",
        "\n",
        "**Late submissions will have a penalty of 10% of the marks for each day of late submission. , and no requests will be answered. Manage your time accordingly.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 1 (10 Points)\n",
        "Describe an interesting research question (or practical question or something innovative) you have in mind, what kind of data should be collected to answer the question(s)? Specify the amount of data needed for analysis. Provide detailed steps for collecting and saving the data."
      ],
      "metadata": {
        "id": "FBKvD6O_TY6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\"Is it possible that the communal emotion of an area impact its everyday power usage?\"\n",
        "\n",
        "#Theory- The hypothesis is that variations in the city's energy consumption could be correlated with the emotional state of the population as a whole, as gathered from multiple social media platforms. For instance, people may use more electricity on really depressing days (emotionally as well as physically), either as a means of self-comfort or as a result of changing their behaviour and staying inside.\n",
        "\n",
        "#Information Required- Social Media Sentiment Data: Gather sentiment ratings from sites like Reddit, Instagram, and Twitter to determine the general attitude of the city. This information ought to contain:The posts' date and time,to make sure the posts are from the desired city, use geolocation, Sentiment ratings (positive, neutral, and negative, for example).\n",
        "#Weather Information: Past weather information to account for how the weather affects energy use-daylight hours, temperature, humidity, and precipitation.\n",
        "#Data on Energy Consumption: The city provides hourly data on energy usage.This ought to have the overall amount of energy used as well as a breakdown by the commercial, industrial, and residential sectors.\n",
        "#Event Data: Significant city events that may have an impact on energy use and mood, such as concerts, sporting events, and protests.Time, place, and nature of the occurrence,Number of Data Required\n",
        "#Sentiment on Social Media: A whole year's worth of data, ideally thousands of posts every day, to capture daily and seasonal fluctuations.Hourly weather information for the same year is provided.\n",
        "#Energy Consumption Data: Hourly data for the same year, if possible, preferably at the minute level of detail.\n",
        "#Event Data: An exhaustive inventory of noteworthy occurrences\n",
        "\n",
        "#Procedures for Gathering and Preserving the Information Social Media Sentiment Analysis:\n",
        "#To compile postings, use APIs from websites like Reddit (Pushshift API) and Twitter (Twitter API).Use geolocation to filter posts and make sure they come from the desired city.To analyse sentiment, use Natural Language Processing (NLP) approaches. Sentiment scores can be obtained using programs like TextBlob or VADER.Store the information in a structured manner with fields for the timestamp, location, and sentiment score, such as a CSV file or database.\n",
        "#Gathering Weather Data:Get weather information from NOAA or OpenWeatherMap, among other sources.To obtain hourly data for the desired city, use APIs.Include the timestamp and weather information in the data and store it in a database or CSV file.\n",
        "#Data Collection on Energy Consumption: Work along with the city's energy supplier to obtain information about use. Use publicly accessible datasets or, if available, extrapolate from smart meter data if direct collaboration isn't feasible. Make that the information is anonymised and conforms with privacy laws.With fields for the timestamp, total consumption, and sector breakdown, save the data in a database or CSV file.\n",
        "#Gathering Event Data:Create an event list by hand using social media, city event calendars, and news sources.Add information such as the event's date, location, and nature.Store this information in a database or CSV file.\n",
        "#Data Organisation and Storage:For ease of access and analysis, keep all datasets in a single, centralised database (such as PostgreSQL or MySQL).If you have access to minute-level granularity for the energy consumption data, use a time-series database (such as InfluxDB).To preserve data integrity, make sure you do regular backups and data validation checks.\n",
        "#Privacy of Data and Ethical Issues:To preserve user privacy, anonymise any personal information gathered via social media.Acquire the required authorisations and abide by data protection laws, such as the CCPA or GDPR.\n",
        "#Preprocessing and Data Cleaning:To manage missing values, outliers, and data normalisation, preprocess the data.Sync up the time intervals between various datasets to ensure precise correlation analysis.\n",
        "\n",
        "#This study may shed light on how social behaviour patterns and emotional states influence pragmatic issues like energy use, which may have implications for resource management and urban planning."
      ],
      "metadata": {
        "id": "cikVKDXdTbzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 2 (10 Points)\n",
        "Write Python code to collect a dataset of 1000 samples related to the question discussed in Question 1."
      ],
      "metadata": {
        "id": "E9RqrlwdTfvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def generate_sentiment_scores(num_samples):\n",
        "    sentiment_data = []\n",
        "    for _ in range(num_samples):\n",
        "        timestamp = datetime.now() - timedelta(days=random.randint(0, 365))\n",
        "        sentiment_score = round(random.uniform(-1, 1), 2)\n",
        "        sentiment_data.append({'timestamp': timestamp, 'sentiment_score': sentiment_score})\n",
        "    return sentiment_data\n",
        "\n",
        "def generate_weather_data(num_samples):\n",
        "    weather_data = []\n",
        "    for _ in range(num_samples):\n",
        "        timestamp = datetime.now() - timedelta(days=random.randint(0, 365))\n",
        "        temperature = round(random.uniform(-10, 35), 2)\n",
        "        humidity = random.randint(10, 100)\n",
        "        weather_data.append({'timestamp': timestamp, 'temperature': temperature, 'humidity': humidity})\n",
        "    return weather_data\n",
        "\n",
        "sentiment_samples = generate_sentiment_scores(1000)\n",
        "\n",
        "weather_samples = generate_weather_data(1000)\n",
        "\n",
        "sentiment_df = pd.DataFrame(sentiment_samples)\n",
        "weather_df = pd.DataFrame(weather_samples)\n",
        "\n",
        "sentiment_df.to_csv('sentiment_data.csv', index=False)\n",
        "weather_df.to_csv('weather_data.csv', index=False)\n",
        "\n",
        "print(\"Sentiment data:\")\n",
        "print(sentiment_df.head())\n",
        "\n",
        "print(\"\\nWeather data:\")\n",
        "print(weather_df.head())\n"
      ],
      "metadata": {
        "id": "4XvRknixTh1g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5640eaf3-9f96-42bc-aa9f-64bda7d71bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment data:\n",
            "                   timestamp  sentiment_score\n",
            "0 2024-01-19 19:07:26.961782            -0.49\n",
            "1 2023-10-25 19:07:26.961826             0.71\n",
            "2 2024-04-07 19:07:26.961835            -0.97\n",
            "3 2024-03-04 19:07:26.961841            -0.01\n",
            "4 2024-02-06 19:07:26.961848             0.67\n",
            "\n",
            "Weather data:\n",
            "                   timestamp  temperature  humidity\n",
            "0 2024-02-29 19:07:26.969884        24.55        91\n",
            "1 2023-11-07 19:07:26.969893        20.47        68\n",
            "2 2024-08-03 19:07:26.969898        10.66        34\n",
            "3 2023-12-02 19:07:26.969902        27.37        65\n",
            "4 2023-12-13 19:07:26.969906         4.72        47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03jb4GZsBkBS"
      },
      "source": [
        "## Question 3 (10 Points)\n",
        "Write Python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"XYZ\". The articles should be published in the last 10 years (2014-2024).\n",
        "\n",
        "The following information from the article needs to be collected:\n",
        "\n",
        "(1) Title of the article\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApLZLw16xUMI",
        "outputId": "d7f149dc-a5ed-4ad7-f991-6a5661361008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "query = 'machine learning'\n",
        "max_results = 100\n",
        "\n",
        "url = f'http://export.arxiv.org/api/query?search_query=all:{query}&start=0&max_results={max_results}'\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'xml')\n",
        "\n",
        "entries = soup.find_all('entry')\n",
        "papers_data = []\n",
        "\n",
        "for entry in entries:\n",
        "    title = entry.title.text\n",
        "    authors = ', '.join([author.find('name').text for author in entry.find_all('author')])\n",
        "    summary = entry.summary.text.strip()\n",
        "    published = entry.published.text\n",
        "    link = entry.id.text\n",
        "\n",
        "    papers_data.append({\n",
        "        'Title': title,\n",
        "        'Authors': authors,\n",
        "        'Summary': summary,\n",
        "        'Published': published,\n",
        "        'Link': link\n",
        "    })\n",
        "\n",
        "papers_df = pd.DataFrame(papers_data)\n",
        "papers_df.to_csv('arxiv_papers.csv', index=False)\n",
        "\n",
        "print(\"ArXiv Papers data:\")\n",
        "print(papers_df.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agN4q56NLl8J",
        "outputId": "a9d97445-ccfd-430b-8600-2fb55ef8bb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ArXiv Papers data:\n",
            "                                               Title  \\\n",
            "0   Lecture Notes: Optimization for Machine Learning   \n",
            "1  An Optimal Control View of Adversarial Machine...   \n",
            "2  Minimax deviation strategies for machine learn...   \n",
            "3  Machine Learning for Clinical Predictive Analy...   \n",
            "4  Towards Modular Machine Learning Solution Deve...   \n",
            "\n",
            "                                    Authors  \\\n",
            "0                                Elad Hazan   \n",
            "1                               Xiaojin Zhu   \n",
            "2  Michail Schlesinger, Evgeniy Vodolazskiy   \n",
            "3                             Wei-Hung Weng   \n",
            "4       Samiyuru Menik, Lakshmish Ramaswamy   \n",
            "\n",
            "                                             Summary             Published  \\\n",
            "0  Lecture notes on optimization for machine lear...  2019-09-08T21:49:42Z   \n",
            "1  I describe an optimal control view of adversar...  2018-11-11T14:28:34Z   \n",
            "2  The article is devoted to the problem of small...  2017-07-16T09:15:08Z   \n",
            "3  In this chapter, we provide a brief overview o...  2019-09-19T22:02:00Z   \n",
            "4  Machine learning technologies have demonstrate...  2023-01-23T22:54:34Z   \n",
            "\n",
            "                                Link  \n",
            "0  http://arxiv.org/abs/1909.03550v1  \n",
            "1  http://arxiv.org/abs/1811.04422v1  \n",
            "2  http://arxiv.org/abs/1707.04849v1  \n",
            "3  http://arxiv.org/abs/1909.09246v1  \n",
            "4  http://arxiv.org/abs/2301.09753v1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJDe71iLB616"
      },
      "source": [
        "## Question 4A (10 Points)\n",
        "Develop Python code to collect data from social media platforms like Reddit, Instagram, Twitter (formerly known as X), Facebook, or any other. Use hashtags, keywords, usernames, or user IDs to gather the data.\n",
        "\n",
        "\n",
        "\n",
        "Ensure that the collected data has more than four columns.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install asyncpraw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pL6Dx9iWk2OM",
        "outputId": "e585d692-cd12-4b8b-f727-1fdf1a43e92b"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: asyncpraw in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Requirement already satisfied: aiofiles<1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.8.0)\n",
            "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (3.10.5)\n",
            "Requirement already satisfied: aiosqlite<=0.17.0 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.17.0)\n",
            "Requirement already satisfied: asyncprawcore<3,>=2.1 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.10/dist-packages (from asyncpraw) (0.18.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4->asyncpraw) (4.0.3)\n",
            "Requirement already satisfied: typing_extensions>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from aiosqlite<=0.17.0->asyncpraw) (4.12.2)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from update-checker>=0.18->asyncpraw) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.3.0->update-checker>=0.18->asyncpraw) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE90z8QqlG15",
        "outputId": "f9778a4c-775b-493b-96cc-da729a1ff0b4"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import asyncpraw\n",
        "import nest_asyncio\n",
        "import asyncio\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "async def fetch_reddit_data():\n",
        "    client_id = 'your_client_id'\n",
        "    client_secret = 'your_client_secret'\n",
        "    user_agent = 'your_user_agent'\n",
        "\n",
        "reddit = asyncpraw.Reddit(client_id=client_id,\n",
        "                          client_secret=client_secret,\n",
        "                          user_agent=user_agent)\n",
        "try:\n",
        "    await reddit.user.me()\n",
        "    print(\"Authentication successful!\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed: {e}\")\n",
        "\n",
        "    data = []\n",
        "    async for submission in subreddit.search(keyword, limit=10):\n",
        "        data.append({\n",
        "            'Title': submission.title,\n",
        "            'Score': submission.score,\n",
        "            'URL': submission.url,\n",
        "            'Comments': submission.num_comments\n",
        "        })\n",
        "\n",
        "    await reddit.close()\n",
        "\n",
        "    for post in data:\n",
        "        print(post)\n",
        "\n",
        "await fetch_reddit_data()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7trhWJA2k2Bp",
        "outputId": "e51732b7-ff89-4cd1-c2fd-4fcab09a37fc"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authentication successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install instaloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1nKKyfcnCId",
        "outputId": "cd406170-2ab7-4d5e-da60-a79cc96eb675"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: instaloader in /usr/local/lib/python3.10/dist-packages (4.13.1)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.10/dist-packages (from instaloader) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import instaloader\n",
        "\n",
        "L = instaloader.Instaloader()\n",
        "username = 'kavya_likitha'\n",
        "password = 'Kavya2703'\n",
        "\n",
        "L.login(username, password)\n",
        "\n",
        "hashtag = 'nature'\n",
        "\n",
        "data = []\n",
        "try:\n",
        "    for post in instaloader.Hashtag.from_name(L.context, hashtag).get_posts():\n",
        "        data.append({\n",
        "            'Post URL': post.url,\n",
        "            'Likes': post.likes,\n",
        "            'Comments': post.comments,\n",
        "            'Caption': post.caption,\n",
        "            'Date': post.date\n",
        "        })\n",
        "        if len(data) >= 10:  # Limit to 10 posts\n",
        "            break\n",
        "\n",
        "except instaloader.exceptions.QueryReturnedNotFoundException as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "for post in data:\n",
        "    print(post)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVTskrA7m6M0",
        "outputId": "38f0cd3d-2aeb-4c39-9a1e-560011dbe23a"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "JSON Query to explore/tags/nature/: 404 Not Found when accessing https://www.instagram.com/explore/tags/nature/?__a=1&__d=dis [retrying; skip with ^C]\n",
            "JSON Query to explore/tags/nature/: 404 Not Found when accessing https://www.instagram.com/explore/tags/nature/?__a=1&__d=dis [retrying; skip with ^C]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: JSON Query to explore/tags/nature/: 404 Not Found when accessing https://www.instagram.com/explore/tags/nature/?__a=1&__d=dis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade instaloader"
      ],
      "metadata": {
        "id": "ZxIvDbwgy9R5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acc248f1-ca3f-4a3e-cc5e-d7b45e96b3c2"
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: instaloader in /usr/local/lib/python3.10/dist-packages (4.13.1)\n",
            "Requirement already satisfied: requests>=2.4 in /usr/local/lib/python3.10/dist-packages (from instaloader) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.4->instaloader) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "\n",
        "reddit = praw.Reddit(\n",
        "    client_id='YOUR_CLIENT_ID',\n",
        "    client_secret='YOUR_CLIENT_SECRET',\n",
        "    user_agent='YOUR_USER_AGENT'\n",
        ")\n",
        "\n",
        "subreddit_name = 'python'\n",
        "search_term = 'data science'\n",
        "post_limit = 100\n",
        "\n",
        "posts_data = []\n",
        "\n",
        "for submission in reddit.subreddit(subreddit_name).search(search_term, limit=post_limit):\n",
        "    posts_data.append({\n",
        "        'Title': submission.title,\n",
        "        'Author': submission.author.name if submission.author else 'N/A',\n",
        "        'Score': submission.score,\n",
        "        'URL': submission.url,\n",
        "        'Content': submission.selftext\n",
        "    })\n",
        "\n",
        "posts_df = pd.DataFrame(posts_data)\n",
        "posts_df.to_csv('reddit_data.csv', index=False)\n",
        "\n",
        "print(\"Reddit data:\")\n",
        "print(posts_df.head())\n"
      ],
      "metadata": {
        "id": "2qiDJTUrpu9W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b5b4e76-f50f-44ec-f076-d7ad01db990f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "JSON Query to explore/tags/picture/: 404 Not Found when accessing https://www.instagram.com/explore/tags/picture/?__a=1&__d=dis [retrying; skip with ^C]\n",
            "JSON Query to explore/tags/picture/: 404 Not Found when accessing https://www.instagram.com/explore/tags/picture/?__a=1&__d=dis [retrying; skip with ^C]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An error occurred: JSON Query to explore/tags/picture/: 404 Not Found when accessing https://www.instagram.com/explore/tags/picture/?__a=1&__d=dis\n",
            "Instagram data:\n",
            "Empty DataFrame\n",
            "Columns: []\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55W9AMdXCSpV"
      },
      "source": [
        "## Question 4B (10 Points)\n",
        "If you encounter challenges with Question-4 web scraping using Python, employ any online tools such as ParseHub or Octoparse for data extraction. Introduce the selected tool, outline the steps for web scraping, and showcase the final output in formats like CSV or Excel.\n",
        "\n",
        "\n",
        "\n",
        "Upload a document (Word or PDF File) in any shared storage (preferably UNT OneDrive) and add the publicly accessible link in the below code cell.\n",
        "\n",
        "Please only choose one option for question 4. If you do both options, we will grade only the first one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I57NXsauCec2"
      },
      "outputs": [],
      "source": [
        "# Used ParseHub to extract the data and the project was successful but unable to download the csv/excel file due to some legal issues of the site.. attached the proofs in the comment section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question"
      ],
      "metadata": {
        "id": "sZOhks1dXWEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important: Reflective Feedback on Web Scraping and Data Collection**\n",
        "\n",
        "\n",
        "\n",
        "Please share your thoughts and feedback on the web scraping and data collection exercises you have completed in this assignment. Consider the following points in your response:\n",
        "\n",
        "\n",
        "\n",
        "Learning Experience: Describe your overall learning experience in working on web scraping tasks. What were the key concepts or techniques you found most beneficial in understanding the process of extracting data from various online sources?\n",
        "\n",
        "\n",
        "\n",
        "Challenges Encountered: Were there specific difficulties in collecting data from certain websites, and how did you overcome them? If you opted for the non-coding option, share your experience with the chosen tool.\n",
        "\n",
        "\n",
        "\n",
        "Relevance to Your Field of Study: How might the ability to gather and analyze data from online sources enhance your work or research?\n",
        "\n",
        "**(no grading of your submission if this question is left unanswered)**"
      ],
      "metadata": {
        "id": "eqmHVEwaWhbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Reflections on Web Scraping and Data Collection\n",
        "\n",
        "#Learning Experience: Diving into web scraping has been a fascinating journey into the realm of data extraction. I discovered that understanding HTML and CSS structures is crucial for navigating web pages effectively. Techniques like using BeautifulSoup for parsing and `requests` for handling HTTP requests became essential tools in my toolkit. The experience highlighted the importance of adapting to different website layouts and learning to handle dynamic content, which proved invaluable in mastering the nuances of scraping.\n",
        "\n",
        "#Challenges Encountered: One notable challenge was dealing with sites that employ dynamic content loading, which sometimes rendered static scraping methods ineffective. For example, handling JavaScript-heavy sites required switching to Selenium for browser automation. Additionally, websites often employ measures to prevent scraping, such as CAPTCHA or rate limiting, which necessitated implementing thoughtful delays and retry logic to mitigate these barriers.\n",
        "\n",
        "#Relevance to Your Field of Study: For a data science student, the ability to gather and analyze data from various online sources opens up a treasure trove of research opportunities. This skill enhances the ability to perform comprehensive analyses and gain insights from real-time data. Whether it’s for academic research, market analysis, or data-driven decision-making, mastering web scraping provides a robust foundation for extracting valuable information from the vast expanse of the web.\n"
      ],
      "metadata": {
        "id": "akAVJn9YBTQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kmmFOz2D1v2V"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "FBKvD6O_TY6e",
        "E9RqrlwdTfvl",
        "03jb4GZsBkBS",
        "jJDe71iLB616",
        "55W9AMdXCSpV",
        "4ulBZ6yhCi9F",
        "6SmvS7nSfbj8",
        "sZOhks1dXWEe"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}