{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ryk8D1Q4Wsrp"
      },
      "source": [
        "# **INFO5731 Assignment 2**\n",
        "\n",
        "In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.\n",
        "\n",
        "**Expectations**:\n",
        "*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.\n",
        "*   Write complete answers and run all the cells before submission.\n",
        "*   Make sure the submission is \"clean\"; *i.e.*, no unnecessary code cells.\n",
        "*   Once finished, allow shared rights from top right corner (*see Canvas for details*).\n",
        "\n",
        "* **Make sure to submit the cleaned data CSV in the comment section - 10 points**\n",
        "\n",
        "**Total points**: 100\n",
        "\n",
        "**Deadline**: Tuesday, at 11:59 PM.\n",
        "\n",
        "**Late Submission will have a penalty of 10% reduction for each day after the deadline.**\n",
        "\n",
        "**Please check that the link you submitted can be opened and points to the correct assignment.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkzR8cFAyGik"
      },
      "source": [
        "# Question 1 (40 points)\n",
        "\n",
        "Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**\n",
        "\n",
        "(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]\n",
        "\n",
        "(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]\n",
        "\n",
        "(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.\n",
        "\n",
        "(4) Collect the **abstracts** of the top 10000 research papers by using the query \"machine learning\", \"data science\", \"artifical intelligence\", or \"information extraction\" from Semantic Scholar.\n",
        "\n",
        "(5) Collect all the information of the 904 narrators in the Densho Digital Repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDyTKYs-yGit",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d15a9a4-a54b-441b-9390-a1ee746a0f8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping page 1...\n",
            "Scraping page 2...\n",
            "Scraping page 3...\n",
            "Scraping page 4...\n",
            "Scraping page 5...\n",
            "Scraping page 6...\n",
            "Scraping page 7...\n",
            "Scraping page 8...\n",
            "Scraping page 9...\n",
            "Scraping page 10...\n",
            "Scraping page 11...\n",
            "Scraping page 12...\n",
            "Scraping page 13...\n",
            "Scraping page 14...\n",
            "Scraping page 15...\n",
            "Scraping page 16...\n",
            "Scraping page 17...\n",
            "Scraping page 18...\n",
            "Scraping page 19...\n",
            "Scraping page 20...\n",
            "Scraping page 21...\n",
            "Scraping page 22...\n",
            "Scraping page 23...\n",
            "Scraping page 24...\n",
            "Scraping page 25...\n",
            "Scraping page 26...\n",
            "Scraping page 27...\n",
            "Scraping page 28...\n",
            "Scraping page 29...\n",
            "Scraping page 30...\n",
            "Scraping page 31...\n",
            "Scraping page 32...\n",
            "Scraping page 33...\n",
            "Scraping page 34...\n",
            "Scraping page 35...\n",
            "Scraping page 36...\n",
            "Scraping page 37...\n",
            "Scraping page 38...\n",
            "Scraping page 39...\n",
            "Scraping page 40...\n",
            "Scraping page 41...\n",
            "Scraping page 42...\n",
            "Scraping page 43...\n",
            "Scraping page 44...\n",
            "Scraping page 45...\n",
            "Scraping page 46...\n",
            "Scraping page 47...\n",
            "Scraping page 48...\n",
            "Scraping page 49...\n",
            "Scraping page 50...\n",
            "Scraping page 51...\n",
            "Scraping page 52...\n",
            "Scraping page 53...\n",
            "Scraping page 54...\n",
            "Scraping page 55...\n",
            "Scraping page 56...\n",
            "Scraping page 57...\n",
            "Scraping page 58...\n",
            "Scraping page 59...\n",
            "Scraping page 60...\n",
            "Scraping page 61...\n",
            "Scraping page 62...\n",
            "Scraping page 63...\n",
            "Scraping page 64...\n",
            "Scraping page 65...\n",
            "Scraping page 66...\n",
            "Scraping page 67...\n",
            "Scraping page 68...\n",
            "Scraping page 69...\n",
            "Scraping page 70...\n",
            "Scraping page 71...\n",
            "Scraping page 72...\n",
            "Scraping page 73...\n",
            "Scraping page 74...\n",
            "Scraping page 75...\n",
            "Scraping page 76...\n",
            "Scraping page 77...\n",
            "Scraping page 78...\n",
            "Scraping page 79...\n",
            "Successfully scraped and saved 1000 reviews to imdb_movie_reviews.csv\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import time\n",
        "\n",
        "# Movie URL on IMDB for the movie Oppenheimer released in 2023 which has around 4k+ reviews\n",
        "url_of_the_movie = \"https://www.imdb.com/title/tt15398776/reviews/?ref_=tt_ov_ql_2\"\n",
        "\n",
        "# Headers for the browser request\n",
        "request_headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36'\n",
        "}\n",
        "\n",
        "# List to store the reviews\n",
        "reviews_list = []\n",
        "\n",
        "# Function to scrape reviews from a url\n",
        "def scrape_reviews(url,request_headers):\n",
        "    response = requests.get(url, headers=request_headers)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Finding all review containers\n",
        "    review_containers = soup.find_all('div', class_='text show-more__control')\n",
        "\n",
        "    for review in review_containers:\n",
        "        reviews_list.append(review.get_text(strip=True))\n",
        "\n",
        "# Iterating through the review pages\n",
        "for page in range(1, 110):  # assuming 10 reviews per page is the standard\n",
        "    print(f\"Scraping page {page}...\")\n",
        "    scrape_reviews(url_of_the_movie + f\"&page={page}\",request_headers)\n",
        "\n",
        "    # Adjusting request throttle and pause to avoid overloading the server\n",
        "    time.sleep(2)\n",
        "\n",
        "    if len(reviews_list) >= 1000:\n",
        "        break\n",
        "\n",
        "# Saving the reviews to a CSV file\n",
        "csv_file = \"imdb_movie_reviews.csv\"\n",
        "\n",
        "with open(csv_file, mode='w', newline='', encoding='utf-8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow([\"Review\"])\n",
        "\n",
        "    for review in reviews_list[:1000]:  # Limiting to 1000 reviews\n",
        "        writer.writerow([review])\n",
        "\n",
        "print(f\"Successfully scraped and saved {len(reviews_list[:1000])} reviews to {csv_file}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90_NR8c5XGWc"
      },
      "source": [
        "# Question 2 (30 points)\n",
        "\n",
        "Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]\n",
        "\n",
        "(1) Remove noise, such as special characters and punctuations.\n",
        "\n",
        "(2) Remove numbers.\n",
        "\n",
        "(3) Remove stopwords by using the stopwords list.\n",
        "\n",
        "(4) Lowercase all texts\n",
        "\n",
        "(5) Stemming.\n",
        "\n",
        "(6) Lemmatization."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2N8DYHalvzL",
        "outputId": "d3407a5e-c36b-48a4-d9f3-95f6a4116b85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QX6bJjGWXY9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "041728c9-8bf9-465a-cf35-947162060936"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data cleaning completed and saved to imdb_movie_reviews_cleaned.csv\n"
          ]
        }
      ],
      "source": [
        "# Write code for each of the sub parts with proper comments.\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Downloading required NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Loading the CSV file with the reviews collected\n",
        "df = pd.read_csv('imdb_movie_reviews.csv')\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # (1) Removing noise (special characters, punctuations, etc.)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # (2) Removing numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # (3) Tokenize and Removing stopwords\n",
        "    words = text.split()  # Tokenize by splitting on spaces\n",
        "    words = [word for word in words if word.lower() not in stop_words]  # Removing stopwords\n",
        "\n",
        "    # (4) Lowercaseing all texts\n",
        "    words = [word.lower() for word in words]\n",
        "\n",
        "    # (5) Stemming\n",
        "    words_stemmed = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    # (6) Lemmatization\n",
        "    words_lemmatized = [lemmatizer.lemmatize(word) for word in words_stemmed]\n",
        "\n",
        "    return ' '.join(words_lemmatized)  # Return cleaned text\n",
        "\n",
        "# Applying the cleaning function to the 'Review' column and saving them in a new column 'Cleaned Review'\n",
        "df['Cleaned Review'] = df['Review'].apply(clean_text)\n",
        "\n",
        "# Saving the updated DataFrame with the new column back as a CSV\n",
        "cleaned_csv = 'imdb_movie_reviews_cleaned.csv'\n",
        "df.to_csv(cleaned_csv, index=False)\n",
        "\n",
        "print(f\"Data cleaning completed and saved to {cleaned_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F_PZdH9Sh49"
      },
      "source": [
        "# Question 3 (30 points)\n",
        "\n",
        "Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:\n",
        "\n",
        "(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.\n",
        "\n",
        "(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.\n",
        "\n",
        "(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas spacy nltk benepar\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_xyCPaKpL8r",
        "outputId": "3686cd87-15d2-4e8f-aa78-adc227b4b4cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: benepar in /usr/local/lib/python3.10/dist-packages (0.2.0)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from benepar) (2.4.1+cu121)\n",
            "Requirement already satisfied: torch-struct>=0.5 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.5)\n",
            "Requirement already satisfied: tokenizers>=0.9.4 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.19.1)\n",
            "Requirement already satisfied: transformers>=4.2.2 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (4.44.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from benepar) (3.20.3)\n",
            "Requirement already satisfied: sentencepiece>=0.1.91 in /usr/local/lib/python3.10/dist-packages (from benepar) (0.2.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.9.4->benepar) (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->benepar) (2024.6.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.2.2->transformers[tokenizers,torch]>=4.2.2->benepar) (0.4.5)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[tokenizers,torch]>=4.2.2->benepar) (0.34.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[tokenizers,torch]>=4.2.2->benepar) (5.9.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->benepar) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.9.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.8.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.19.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0oOSlsOS0cq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf13e578-2e85-410a-bc6e-aeb0e08584c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package benepar_en3 to /root/nltk_data...\n",
            "[nltk_data]   Package benepar_en3 is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/benepar/parse_chart.py:169: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Constituency Parsing Tree ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/distributions/distribution.py:55: UserWarning: <class 'torch_struct.distributions.TreeCRF'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TOP\n",
            "  (S\n",
            "    (MD youll)\n",
            "    (VB wit)\n",
            "    (NP (NN brain) (JJ fulli) (NN switch))\n",
            "    (VB watch)\n",
            "    (NP (NNP oppenheim))\n",
            "    (VP\n",
            "      (MD could)\n",
            "      (VP\n",
            "        (NP (PRP easili))\n",
            "        (VB get)\n",
            "        (PRT (RP away))\n",
            "        (NP (JJ nonattent) (NN viewer))\n",
            "        (JJ intellig)\n",
            "        (NN filmmak)\n",
            "        (NN show)\n",
            "        (FW audienc)\n",
            "        (JJ great)\n",
            "        (NN respect)\n",
            "        (NN fire)\n",
            "        (FW dialogu)\n",
            "        (NN pack)\n",
            "        (VB inform)\n",
            "        (JJ relentless)\n",
            "        (NN pace)\n",
            "        (NN jump)\n",
            "        (VB differ)\n",
            "        (NN time)\n",
            "        (FW oppenheim)\n",
            "        (NN life)\n",
            "        (NP (JJ continu) (NN hour))\n",
            "        (JJ runtim)\n",
            "        (JJ visual)\n",
            "        (NN clue)\n",
            "        (JJ guid)\n",
            "        (NN viewer)\n",
            "        (NN time)\n",
            "        (MD youll)\n",
            "        (VP\n",
            "          (VB get)\n",
            "          (NP (NN grip))\n",
            "          (VB quit)\n",
            "          (FW quickli)\n",
            "          (JJ relentless)\n",
            "          (NN help)\n",
            "          (VB express)\n",
            "          (NN urgenc)\n",
            "          (FW u)\n",
            "          (NN attack)\n",
            "          (NN chase)\n",
            "          (NN atom)\n",
            "          (NN bomb)\n",
            "          (FW germani)\n",
            "          (VP\n",
            "            (MD could)\n",
            "            (RB absolut)\n",
            "            (NN career)\n",
            "            (RBS best)\n",
            "            (VB perform)\n",
            "            (NN consistenli)\n",
            "            (JJ brilliant)\n",
            "            (JJ cillian)\n",
            "            (NN murphi)\n",
            "            (NN anchor)\n",
            "            (NN film)\n",
            "            (NN nail)\n",
            "            (NN oscar)\n",
            "            (VB perform)\n",
            "            (NP (NP (VB fact)) (RB whole) (VBN cast) (RB fantast))\n",
            "            (VP\n",
            "              (RB apart)\n",
            "              (VB mayb)\n",
            "              (RB sometim)\n",
            "              (VP\n",
            "                (NP\n",
            "                  (VBN overwrought)\n",
            "                  (RB emili)\n",
            "                  (JJ blunt)\n",
            "                  (VB perform)\n",
            "                  (NP (NN rdj)))\n",
            "                (ADVP (RB also))\n",
            "                (. particularli)\n",
            "                (JJ brilliant)\n",
            "                (VB return)\n",
            "                (ADJP\n",
            "                  (JJ proper)\n",
            "                  (NN act)\n",
            "                  (VP\n",
            "                    (VBN decad)\n",
            "                    (VB call)\n",
            "                    (NN screenplay)\n",
            "                    (DT den)\n",
            "                    (NN layer)))))))))))\n",
            "\n",
            "=== Dependency Parsing Tree ===\n",
            "you (nsubj) <-- wit\n",
            "ll (aux) <-- wit\n",
            "wit (nsubj) <-- absolut\n",
            "brain (compound) <-- switch\n",
            "fulli (compound) <-- switch\n",
            "switch (compound) <-- watch\n",
            "watch (dobj) <-- wit\n",
            "oppenheim (nsubj) <-- easili\n",
            "could (aux) <-- easili\n",
            "easili (ccomp) <-- wit\n",
            "get (conj) <-- wit\n",
            "away (prt) <-- get\n",
            "nonattent (compound) <-- filmmak\n",
            "viewer (compound) <-- intellig\n",
            "intellig (compound) <-- filmmak\n",
            "filmmak (nsubj) <-- show\n",
            "show (conj) <-- wit\n",
            "audienc (nmod) <-- fire\n",
            "great (amod) <-- respect\n",
            "respect (compound) <-- fire\n",
            "fire (nmod) <-- inform\n",
            "dialogu (amod) <-- pack\n",
            "pack (nsubj) <-- inform\n",
            "inform (conj) <-- wit\n",
            "relentless (amod) <-- jump\n",
            "pace (compound) <-- jump\n",
            "jump (dobj) <-- inform\n",
            "differ (conj) <-- wit\n",
            "time (dobj) <-- differ\n",
            "oppenheim (compound) <-- life\n",
            "life (compound) <-- continu\n",
            "continu (nmod) <-- clue\n",
            "hour (nmod) <-- clue\n",
            "runtim (nmod) <-- clue\n",
            "visual (amod) <-- clue\n",
            "clue (compound) <-- guid\n",
            "guid (appos) <-- time\n",
            "viewer (amod) <-- time\n",
            "time (dobj) <-- wit\n",
            "you (nsubj) <-- get\n",
            "ll (aux) <-- get\n",
            "get (relcl) <-- time\n",
            "grip (nsubj) <-- quit\n",
            "quit (ccomp) <-- wit\n",
            "quickli (dobj) <-- quit\n",
            "relentless (amod) <-- help\n",
            "help (aux) <-- express\n",
            "express (conj) <-- wit\n",
            "urgenc (compound) <-- germani\n",
            "u (compound) <-- chase\n",
            "attack (compound) <-- chase\n",
            "chase (compound) <-- germani\n",
            "atom (compound) <-- bomb\n",
            "bomb (compound) <-- germani\n",
            "germani (dobj) <-- express\n",
            "could (aux) <-- absolut\n",
            "absolut (ROOT) <-- absolut\n",
            "career (nmod) <-- oscar\n",
            "best (advmod) <-- career\n",
            "perform (compound) <-- brilliant\n",
            "consistenli (compound) <-- brilliant\n",
            "brilliant (compound) <-- oscar\n",
            "cillian (compound) <-- murphi\n",
            "murphi (compound) <-- oscar\n",
            "anchor (compound) <-- oscar\n",
            "film (compound) <-- oscar\n",
            "nail (compound) <-- oscar\n",
            "oscar (nsubj) <-- perform\n",
            "perform (ccomp) <-- absolut\n",
            "fact (dobj) <-- perform\n",
            "whole (amod) <-- fantast\n",
            "cast (compound) <-- fantast\n",
            "fantast (npadvmod) <-- perform\n",
            "apart (prep) <-- perform\n",
            "mayb (compound) <-- emili\n",
            "sometim (nmod) <-- emili\n",
            "overwrought (compound) <-- emili\n",
            "emili (compound) <-- perform\n",
            "blunt (compound) <-- perform\n",
            "perform (compound) <-- rdj\n",
            "rdj (conj) <-- perform\n",
            "also (advmod) <-- perform\n",
            "particularli (amod) <-- return\n",
            "brilliant (amod) <-- return\n",
            "return (dobj) <-- perform\n",
            "proper (amod) <-- act\n",
            "act (compound) <-- layer\n",
            "decad (compound) <-- call\n",
            "call (compound) <-- layer\n",
            "screenplay (compound) <-- layer\n",
            "den (compound) <-- layer\n",
            "layer (appos) <-- return\n",
            "i (nsubj) <-- say\n",
            "d (nsubj) <-- say\n",
            "say (ROOT) <-- say\n",
            "thick (amod) <-- cinematographi\n",
            "bibl (compound) <-- cinematographi\n",
            "cinematographi (nsubj) <-- quit\n",
            "quit (ccomp) <-- say\n",
            "stark (amod) <-- moment\n",
            "spare (amod) <-- part\n",
            "part (nmod) <-- imbu\n",
            "imbu (nmod) <-- luciou\n",
            "rich (amod) <-- luciou\n",
            "luciou (compound) <-- moment\n",
            "colour (compound) <-- moment\n",
            "moment (dobj) <-- quit\n",
            "especi (ccomp) <-- quit\n",
            "scene (compound) <-- pugh\n",
            "florenc (compound) <-- pugh\n",
            "pugh (compound) <-- beauti\n",
            "score (compound) <-- beauti\n",
            "beauti (compound) <-- time\n",
            "time (npadvmod) <-- especi\n",
            "mostli (nsubj) <-- anxiou\n",
            "anxiou (relcl) <-- time\n",
            "oppress (compound) <-- ad\n",
            "ad (nmod) <-- hour\n",
            "relentless (amod) <-- pace\n",
            "pace (compound) <-- hour\n",
            "hour (dobj) <-- anxiou\n",
            "runtim (amod) <-- fli\n",
            "fli (nsubj) <-- found\n",
            "found (conj) <-- say\n",
            "intens (dobj) <-- found\n",
            "tax (compound) <-- highli\n",
            "highli (compound) <-- reward\n",
            "reward (compound) <-- film\n",
            "watch (compound) <-- film\n",
            "film (nsubj) <-- make\n",
            "make (ccomp) <-- found\n",
            "finest (amod) <-- watch\n",
            "realli (nmod) <-- watch\n",
            "great (amod) <-- watch\n",
            "watch (dobj) <-- make\n",
            "\n",
            "Example Sentence for Parsing: youll wit brain fulli switch watch oppenheim could easili get away nonattent viewer intellig filmmak show audienc great respect fire dialogu pack inform relentless pace jump differ time oppenheim life continu hour runtim visual clue guid viewer time youll get grip quit quickli relentless help express urgenc u attack chase atom bomb germani could absolut career best perform consistenli brilliant cillian murphi anchor film nail oscar perform fact whole cast fantast apart mayb sometim overwrought emili blunt perform rdj also particularli brilliant return proper act decad call screenplay den layer\n",
            "\n",
            "=== POS Tagging Results ===\n",
            "Total Nouns: 95480\n",
            "Total Verbs: 20920\n",
            "Total Adjectives: 36680\n",
            "Total Adverbs: 9040\n",
            "\n",
            "=== Named Entity Recognition Results ===\n",
            "PERSON: 4320\n",
            "ORG: 2240\n",
            "NORP: 1040\n",
            "CARDINAL: 1760\n",
            "DATE: 480\n",
            "TIME: 320\n",
            "ORDINAL: 680\n",
            "GPE: 1560\n",
            "EVENT: 80\n",
            "WORK_OF_ART: 80\n",
            "FAC: 120\n",
            "LOC: 80\n",
            "PRODUCT: 120\n"
          ]
        }
      ],
      "source": [
        "# Your code here\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "import benepar\n",
        "\n",
        "# Loading cleaned data from the CSV file\n",
        "df = pd.read_csv('imdb_movie_reviews_cleaned.csv')\n",
        "\n",
        "# Loading SpaCy model for POS tagging, Dependency Parsing, and NER\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Initializing Benepar for Constituency Parsing\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "benepar.download('benepar_en3')\n",
        "parser = benepar.Parser(\"benepar_en3\")\n",
        "\n",
        "# Function for POS tagging and counting N, V, Adj, Adv\n",
        "def pos_analysis(text):\n",
        "    pos_counts = Counter()  # Dictionary to hold the counts of POS tags\n",
        "    tokens = word_tokenize(text)  # Tokenize the text\n",
        "    tagged_words = pos_tag(tokens)  # POS tagging\n",
        "\n",
        "    for word, tag in tagged_words:\n",
        "        if tag.startswith('N'):  # Noun\n",
        "            pos_counts['Noun'] += 1\n",
        "        elif tag.startswith('V'):  # Verb\n",
        "            pos_counts['Verb'] += 1\n",
        "        elif tag.startswith('JJ'):  # Adjective\n",
        "            pos_counts['Adjective'] += 1\n",
        "        elif tag.startswith('RB'):  # Adverb\n",
        "            pos_counts['Adverb'] += 1\n",
        "\n",
        "    return pos_counts\n",
        "\n",
        "# Function for Constituency Parsing and Dependency Parsing\n",
        "def parse_trees(text):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    print(\"\\n=== Constituency Parsing Tree ===\")\n",
        "    # Applying constituency parsing on the first sentence as an example\n",
        "    sentence = list(doc.sents)[0].text\n",
        "    tree = parser.parse(sentence.split())\n",
        "    print(tree)\n",
        "\n",
        "    print(\"\\n=== Dependency Parsing Tree ===\")\n",
        "    for token in doc:\n",
        "        print(f'{token.text} ({token.dep_}) <-- {token.head.text}')\n",
        "\n",
        "    return sentence\n",
        "\n",
        "# Function for Named Entity Recognition and counting entity types\n",
        "def ner_analysis(text):\n",
        "    doc = nlp(text)\n",
        "    entity_counts = Counter()\n",
        "\n",
        "    for ent in doc.ents:\n",
        "        entity_counts[ent.label_] += 1\n",
        "\n",
        "    return entity_counts\n",
        "\n",
        "# Applying the POS tagging, parsing, and NER on cleaned data\n",
        "all_pos_counts = Counter()\n",
        "all_entity_counts = Counter()\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    clean_text = row['Cleaned Review']\n",
        "\n",
        "    # (1) POS Tagging\n",
        "    pos_counts = pos_analysis(clean_text)\n",
        "    all_pos_counts.update(pos_counts)\n",
        "\n",
        "    # (2) Parsing (only for the first review to avoid too many trees)\n",
        "    if index == 0:\n",
        "        sentence = parse_trees(clean_text)\n",
        "        print(f\"\\nExample Sentence for Parsing: {sentence}\")\n",
        "\n",
        "    # (3) NER\n",
        "    entity_counts = ner_analysis(clean_text)\n",
        "    all_entity_counts.update(entity_counts)\n",
        "\n",
        "# Printing POS counts\n",
        "print(\"\\n=== POS Tagging Results ===\")\n",
        "print(f\"Total Nouns: {all_pos_counts['Noun']}\")\n",
        "print(f\"Total Verbs: {all_pos_counts['Verb']}\")\n",
        "print(f\"Total Adjectives: {all_pos_counts['Adjective']}\")\n",
        "print(f\"Total Adverbs: {all_pos_counts['Adverb']}\")\n",
        "\n",
        "# Printing NER counts\n",
        "print(\"\\n=== Named Entity Recognition Results ===\")\n",
        "for entity, count in all_entity_counts.items():\n",
        "    print(f\"{entity}: {count}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Comment**\n",
        "Make sure to submit the cleaned data CSV in the comment section - 10 points"
      ],
      "metadata": {
        "id": "CXNn1lEVbMsv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qYRO5Cn8bYwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mandatory Question\n",
        "\n",
        "Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment."
      ],
      "metadata": {
        "id": "q8BFCvWp32cf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your response below\n",
        "'''\n",
        "This assignment was a great exercise in both data processing and natural language analysis. The challenges mainly revolved around balancing simplicity with the depth of analysis. For instance, ensuring that each step of the pipeline, from text cleaning to advanced syntax parsing, worked smoothly required integrating several libraries, each with its nuances. Constituency parsing, in particular, was an exciting challenge, as it forced me to consider sentence structures more deeply than I typically would. What I enjoyed most was the Named Entity Recognition (NER) part, as it reveals how much structured information can be extracted from unstructured text. Seeing entities like organizations, dates, and locations surface from the raw data was satisfying. It emphasizes how much insight can be gained from seemingly ordinary text. This kind of task can be time-intensive, especially for students new to NLP or Python. Depending on their familiarity with the libraries used (like `spacy`, `nltk`, and `benepar`), students may need additional time for installation, testing, and debugging. A few extra days would ensure deeper understanding and a more robust analysis.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "_e557s2w4BpK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "947a8937-9cc0-47f8-c645-96de9d6e0736"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nThis assignment was a great exercise in both data processing and natural language analysis. The challenges mainly revolved around balancing simplicity with the depth of analysis. For instance, ensuring that each step of the pipeline, from text cleaning to advanced syntax parsing, worked smoothly required integrating several libraries, each with its nuances. Constituency parsing, in particular, was an exciting challenge, as it forced me to consider sentence structures more deeply than I typically would. What I enjoyed most was the Named Entity Recognition (NER) part, as it reveals how much structured information can be extracted from unstructured text. Seeing entities like organizations, dates, and locations surface from the raw data was satisfying. It emphasizes how much insight can be gained from seemingly ordinary text. This kind of task can be time-intensive, especially for students new to NLP or Python. Depending on their familiarity with the libraries used (like `spacy`, `nltk`, and `benepar`), students may need additional time for installation, testing, and debugging. A few extra days would ensure deeper understanding and a more robust analysis.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "OP4-5qxAtInv"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}